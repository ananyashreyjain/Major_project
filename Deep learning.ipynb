{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from statistics import mean, median\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "tf.disable_eager_execution()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CartPoleEnv - Version 0.2.0, Noise case: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anany/anaconda3/envs/ML/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym.envs.classic_control.cartpole.CartPoleEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "LR = 1e-3\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "TRIALS = 20\n",
    "params = dict(\n",
    "    goal_steps = 500,\n",
    "    score_requirement = 30,\n",
    "    initial_games = 500,\n",
    "    X = np.array([[]]),\n",
    "    Y = np.array([]),\n",
    "    action = 2,\n",
    "    score = np.array([0]),\n",
    "    plot = False,\n",
    "    model = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(scores=[]):\n",
    "    plt.clf()\n",
    "    avg_scores = [sum(scores[:index+1])/(index+1) for index, \n",
    "                  score in enumerate(scores)]\n",
    "    avg_score = sum(scores)/len(scores)\n",
    "    x_val = [0,len(scores)-1]\n",
    "    y_val = [avg_score, avg_score]\n",
    "    plt.plot(scores,'g-', label='current score')\n",
    "    plt.plot(x_val, y_val,'r-', label='average score')\n",
    "    plt.plot(avg_scores,'b-', label='average scores')\n",
    "    plt.xlabel('Episodes--->')\n",
    "    plt.ylabel('Score--->')\n",
    "    plt.legend()\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(params):\n",
    "    env.reset()\n",
    "    for game in range(params[\"initial_games\"]):\n",
    "        env.reset()\n",
    "        game_data = np.array([])\n",
    "        prev_observation = np.array([])\n",
    "        action_list = np.array([])\n",
    "        for step in range(params[\"goal_steps\"]):\n",
    "            if not prev_observation.size or params['model'] is None:\n",
    "                action = env.action_space.sample()\n",
    "            else :\n",
    "                prob_values = model.predict(np.array([prev_observation]))[0]\n",
    "                action = np.argmax(prob_values)\n",
    "            observation, reward, done, info =  env.step(action)\n",
    "            params['score'][-1] += reward\n",
    "            if prev_observation.size :\n",
    "                if not game_data.size:\n",
    "                    game_data = prev_observation\n",
    "                    action_list = np.array([action])\n",
    "                else:\n",
    "                    game_data = np.vstack((game_data, prev_observation))\n",
    "                    action_list = np.concatenate((action_list,action), axis = None)\n",
    "            if done:\n",
    "                break\n",
    "            prev_observation = observation\n",
    "        if params['plot']:\n",
    "            plot(params['score'])\n",
    "        if params['score'][-1] >= params['score_requirement']:\n",
    "            if not params['X'].size:\n",
    "                params['X'] = game_data\n",
    "                params['Y'] = action_list\n",
    "            else:\n",
    "                params['X']=np.concatenate((params['X'], game_data), axis=0)\n",
    "                params['Y']=np.concatenate((params['Y'], action_list))\n",
    "        params['score'] = np.concatenate((params['score'], 0), axis = None)\n",
    "data(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anany/anaconda3/envs/ML/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Train on 931 samples\n",
      "Epoch 1/10\n",
      "931/931 [==============================] - 0s 214us/sample - loss: 0.6902\n",
      "Epoch 2/10\n",
      "931/931 [==============================] - 0s 67us/sample - loss: 0.6783\n",
      "Epoch 3/10\n",
      "931/931 [==============================] - 0s 61us/sample - loss: 0.6737\n",
      "Epoch 4/10\n",
      "931/931 [==============================] - 0s 52us/sample - loss: 0.6696\n",
      "Epoch 5/10\n",
      "931/931 [==============================] - 0s 51us/sample - loss: 0.6659\n",
      "Epoch 6/10\n",
      "931/931 [==============================] - 0s 51us/sample - loss: 0.6603\n",
      "Epoch 7/10\n",
      "931/931 [==============================] - 0s 40us/sample - loss: 0.6569\n",
      "Epoch 8/10\n",
      "931/931 [==============================] - 0s 44us/sample - loss: 0.6563\n",
      "Epoch 9/10\n",
      "931/931 [==============================] - 0s 52us/sample - loss: 0.6530\n",
      "Epoch 10/10\n",
      "931/931 [==============================] - 0s 46us/sample - loss: 0.6498\n",
      "Train on 18963 samples\n",
      "Epoch 1/10\n",
      "18963/18963 [==============================] - 1s 40us/sample - loss: 0.2113\n",
      "Epoch 2/10\n",
      "18963/18963 [==============================] - 1s 34us/sample - loss: 0.0955\n",
      "Epoch 3/10\n",
      "18963/18963 [==============================] - 1s 44us/sample - loss: 0.0737\n",
      "Epoch 4/10\n",
      "18963/18963 [==============================] - 1s 35us/sample - loss: 0.0628\n",
      "Epoch 5/10\n",
      "18963/18963 [==============================] - 1s 35us/sample - loss: 0.0574\n",
      "Epoch 6/10\n",
      "18963/18963 [==============================] - 1s 34us/sample - loss: 0.0529\n",
      "Epoch 7/10\n",
      "18963/18963 [==============================] - 1s 35us/sample - loss: 0.0521\n",
      "Epoch 8/10\n",
      "18963/18963 [==============================] - 1s 36us/sample - loss: 0.0491\n",
      "Epoch 9/10\n",
      "18963/18963 [==============================] - 1s 34us/sample - loss: 0.0465\n",
      "Epoch 10/10\n",
      "18963/18963 [==============================] - 1s 35us/sample - loss: 0.0431\n",
      "Train on 16041 samples\n",
      "Epoch 1/10\n",
      "16041/16041 [==============================] - 1s 48us/sample - loss: 0.0356\n",
      "Epoch 2/10\n",
      "16041/16041 [==============================] - 1s 45us/sample - loss: 0.0347\n",
      "Epoch 3/10\n",
      "16041/16041 [==============================] - 1s 36us/sample - loss: 0.0331\n",
      "Epoch 4/10\n",
      "16041/16041 [==============================] - 1s 37us/sample - loss: 0.0303\n",
      "Epoch 5/10\n",
      "16041/16041 [==============================] - 1s 37us/sample - loss: 0.0354\n",
      "Epoch 6/10\n",
      "16041/16041 [==============================] - 1s 37us/sample - loss: 0.0313\n",
      "Epoch 7/10\n",
      "16041/16041 [==============================] - 1s 49us/sample - loss: 0.0330\n",
      "Epoch 8/10\n",
      "16041/16041 [==============================] - 1s 46us/sample - loss: 0.0324\n",
      "Epoch 9/10\n",
      "16041/16041 [==============================] - 1s 44us/sample - loss: 0.0317\n",
      "Epoch 10/10\n",
      "16041/16041 [==============================] - 1s 52us/sample - loss: 0.0291\n",
      "Train on 16995 samples\n",
      "Epoch 1/10\n",
      "16995/16995 [==============================] - 1s 44us/sample - loss: 0.0305\n",
      "Epoch 2/10\n",
      "16995/16995 [==============================] - 1s 38us/sample - loss: 0.0322\n",
      "Epoch 3/10\n",
      "16995/16995 [==============================] - 1s 36us/sample - loss: 0.0285\n",
      "Epoch 4/10\n",
      "16995/16995 [==============================] - 1s 37us/sample - loss: 0.0271\n",
      "Epoch 5/10\n",
      "16995/16995 [==============================] - 1s 36us/sample - loss: 0.0311\n",
      "Epoch 6/10\n",
      "16995/16995 [==============================] - 1s 37us/sample - loss: 0.0290\n",
      "Epoch 7/10\n",
      "16995/16995 [==============================] - 1s 38us/sample - loss: 0.0260\n",
      "Epoch 8/10\n",
      "16995/16995 [==============================] - 1s 36us/sample - loss: 0.0288\n",
      "Epoch 9/10\n",
      "16995/16995 [==============================] - 1s 40us/sample - loss: 0.0289\n",
      "Epoch 10/10\n",
      "16995/16995 [==============================] - 1s 36us/sample - loss: 0.0250\n",
      "Train on 17049 samples\n",
      "Epoch 1/10\n",
      "17049/17049 [==============================] - 1s 46us/sample - loss: 0.0277\n",
      "Epoch 2/10\n",
      "17049/17049 [==============================] - 1s 38us/sample - loss: 0.0239\n",
      "Epoch 3/10\n",
      "17049/17049 [==============================] - 1s 38us/sample - loss: 0.0286\n",
      "Epoch 4/10\n",
      "17049/17049 [==============================] - 1s 38us/sample - loss: 0.0274\n",
      "Epoch 5/10\n",
      "17049/17049 [==============================] - 1s 38us/sample - loss: 0.0267\n",
      "Epoch 6/10\n",
      "17049/17049 [==============================] - 1s 39us/sample - loss: 0.0241\n",
      "Epoch 7/10\n",
      "17049/17049 [==============================] - 1s 38us/sample - loss: 0.0242\n",
      "Epoch 8/10\n",
      "17049/17049 [==============================] - 1s 56us/sample - loss: 0.02390s - los\n",
      "Epoch 9/10\n",
      "17049/17049 [==============================] - 1s 41us/sample - loss: 0.0272\n",
      "Epoch 10/10\n",
      "17049/17049 [==============================] - 1s 39us/sample - loss: 0.0225\n",
      "Train on 16105 samples\n",
      "Epoch 1/10\n",
      "16105/16105 [==============================] - 1s 49us/sample - loss: 0.0283\n",
      "Epoch 2/10\n",
      "16105/16105 [==============================] - 1s 40us/sample - loss: 0.0332\n",
      "Epoch 3/10\n",
      "16105/16105 [==============================] - 1s 44us/sample - loss: 0.0267\n",
      "Epoch 4/10\n",
      "16105/16105 [==============================] - 1s 47us/sample - loss: 0.0260\n",
      "Epoch 5/10\n",
      "16105/16105 [==============================] - 1s 46us/sample - loss: 0.0231\n",
      "Epoch 6/10\n",
      "16105/16105 [==============================] - 1s 46us/sample - loss: 0.0259\n",
      "Epoch 7/10\n",
      "16105/16105 [==============================] - 1s 44us/sample - loss: 0.0280\n",
      "Epoch 8/10\n",
      "16105/16105 [==============================] - 1s 44us/sample - loss: 0.0228\n",
      "Epoch 9/10\n",
      "16105/16105 [==============================] - 1s 44us/sample - loss: 0.0233\n",
      "Epoch 10/10\n",
      "16105/16105 [==============================] - 1s 50us/sample - loss: 0.0240\n",
      "Train on 13496 samples\n",
      "Epoch 1/10\n",
      "13496/13496 [==============================] - 1s 54us/sample - loss: 0.0293\n",
      "Epoch 2/10\n",
      "13496/13496 [==============================] - 1s 40us/sample - loss: 0.0220\n",
      "Epoch 3/10\n",
      "13496/13496 [==============================] - 1s 51us/sample - loss: 0.0197\n",
      "Epoch 4/10\n",
      "13496/13496 [==============================] - 1s 47us/sample - loss: 0.0212\n",
      "Epoch 5/10\n",
      "13496/13496 [==============================] - 1s 43us/sample - loss: 0.0196\n",
      "Epoch 6/10\n",
      "13496/13496 [==============================] - 1s 42us/sample - loss: 0.0248\n",
      "Epoch 7/10\n",
      "13496/13496 [==============================] - 1s 47us/sample - loss: 0.0209\n",
      "Epoch 8/10\n",
      "13496/13496 [==============================] - 1s 43us/sample - loss: 0.0220\n",
      "Epoch 9/10\n",
      "13496/13496 [==============================] - 1s 48us/sample - loss: 0.0198\n",
      "Epoch 10/10\n",
      "13496/13496 [==============================] - 1s 44us/sample - loss: 0.0209\n",
      "Train on 11550 samples\n",
      "Epoch 1/10\n",
      "11550/11550 [==============================] - 1s 82us/sample - loss: 0.0239\n",
      "Epoch 2/10\n",
      "11550/11550 [==============================] - 1s 46us/sample - loss: 0.0184\n",
      "Epoch 3/10\n",
      "11550/11550 [==============================] - 1s 48us/sample - loss: 0.0208\n",
      "Epoch 4/10\n",
      "11550/11550 [==============================] - 0s 42us/sample - loss: 0.0197\n",
      "Epoch 5/10\n",
      "11550/11550 [==============================] - 1s 44us/sample - loss: 0.0194\n",
      "Epoch 6/10\n",
      "11550/11550 [==============================] - 1s 49us/sample - loss: 0.0230\n",
      "Epoch 7/10\n",
      "11550/11550 [==============================] - 1s 46us/sample - loss: 0.0213\n",
      "Epoch 8/10\n",
      "11550/11550 [==============================] - 1s 46us/sample - loss: 0.0177\n",
      "Epoch 9/10\n",
      "11550/11550 [==============================] - 1s 46us/sample - loss: 0.0208\n",
      "Epoch 10/10\n",
      "11550/11550 [==============================] - 1s 49us/sample - loss: 0.0186\n",
      "Train on 6660 samples\n",
      "Epoch 1/10\n",
      "6660/6660 [==============================] - 1s 77us/sample - loss: 0.0276\n",
      "Epoch 2/10\n",
      "6660/6660 [==============================] - 0s 42us/sample - loss: 0.0215\n",
      "Epoch 3/10\n",
      "6660/6660 [==============================] - 0s 44us/sample - loss: 0.0212\n",
      "Epoch 4/10\n",
      "6660/6660 [==============================] - 0s 53us/sample - loss: 0.0188\n",
      "Epoch 5/10\n",
      "6660/6660 [==============================] - 0s 49us/sample - loss: 0.0281\n",
      "Epoch 6/10\n",
      "6660/6660 [==============================] - 0s 52us/sample - loss: 0.0273\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6660/6660 [==============================] - 0s 59us/sample - loss: 0.0200\n",
      "Epoch 8/10\n",
      "6660/6660 [==============================] - 0s 42us/sample - loss: 0.0226\n",
      "Epoch 9/10\n",
      "6660/6660 [==============================] - 0s 58us/sample - loss: 0.0216\n",
      "Epoch 10/10\n",
      "6660/6660 [==============================] - 0s 52us/sample - loss: 0.0178\n",
      "Train on 3834 samples\n",
      "Epoch 1/10\n",
      "3834/3834 [==============================] - 0s 101us/sample - loss: 0.0284\n",
      "Epoch 2/10\n",
      "3834/3834 [==============================] - 0s 39us/sample - loss: 0.0199\n",
      "Epoch 3/10\n",
      "3834/3834 [==============================] - 0s 39us/sample - loss: 0.0216\n",
      "Epoch 4/10\n",
      "3834/3834 [==============================] - 0s 40us/sample - loss: 0.0241\n",
      "Epoch 5/10\n",
      "3834/3834 [==============================] - 0s 57us/sample - loss: 0.0149\n",
      "Epoch 6/10\n",
      "3834/3834 [==============================] - 0s 80us/sample - loss: 0.0170\n",
      "Epoch 7/10\n",
      "3834/3834 [==============================] - 0s 47us/sample - loss: 0.0158\n",
      "Epoch 8/10\n",
      "3834/3834 [==============================] - 0s 41us/sample - loss: 0.0146\n",
      "Epoch 9/10\n",
      "3834/3834 [==============================] - 0s 48us/sample - loss: 0.0202\n",
      "Epoch 10/10\n",
      "3834/3834 [==============================] - 0s 63us/sample - loss: 0.0151\n",
      "Train on 3965 samples\n",
      "Epoch 1/10\n",
      "3965/3965 [==============================] - 0s 106us/sample - loss: 0.0259\n",
      "Epoch 2/10\n",
      "3965/3965 [==============================] - 0s 43us/sample - loss: 0.0183\n",
      "Epoch 3/10\n",
      "3965/3965 [==============================] - 0s 42us/sample - loss: 0.0196\n",
      "Epoch 4/10\n",
      "3965/3965 [==============================] - 0s 42us/sample - loss: 0.0213\n",
      "Epoch 5/10\n",
      "3965/3965 [==============================] - 0s 45us/sample - loss: 0.0191\n",
      "Epoch 6/10\n",
      "3965/3965 [==============================] - 0s 43us/sample - loss: 0.0213\n",
      "Epoch 7/10\n",
      "3965/3965 [==============================] - 0s 70us/sample - loss: 0.0138\n",
      "Epoch 8/10\n",
      "3965/3965 [==============================] - 0s 46us/sample - loss: 0.0200\n",
      "Epoch 9/10\n",
      "3965/3965 [==============================] - 0s 47us/sample - loss: 0.0352\n",
      "Epoch 10/10\n",
      "3965/3965 [==============================] - 0s 49us/sample - loss: 0.0218\n",
      "Train on 2519 samples\n",
      "Epoch 1/10\n",
      "2519/2519 [==============================] - 0s 151us/sample - loss: 0.0317\n",
      "Epoch 2/10\n",
      "2519/2519 [==============================] - 0s 43us/sample - loss: 0.0186\n",
      "Epoch 3/10\n",
      "2519/2519 [==============================] - 0s 47us/sample - loss: 0.0178\n",
      "Epoch 4/10\n",
      "2519/2519 [==============================] - 0s 47us/sample - loss: 0.0235\n",
      "Epoch 5/10\n",
      "2519/2519 [==============================] - 0s 45us/sample - loss: 0.0320\n",
      "Epoch 6/10\n",
      "2519/2519 [==============================] - 0s 53us/sample - loss: 0.0239\n",
      "Epoch 7/10\n",
      "2519/2519 [==============================] - 0s 74us/sample - loss: 0.0137\n",
      "Epoch 8/10\n",
      "2519/2519 [==============================] - 0s 46us/sample - loss: 0.0143\n",
      "Epoch 9/10\n",
      "2519/2519 [==============================] - 0s 49us/sample - loss: 0.0137\n",
      "Epoch 10/10\n",
      "2519/2519 [==============================] - 0s 51us/sample - loss: 0.0179\n",
      "Train on 1870 samples\n",
      "Epoch 1/10\n",
      "1870/1870 [==============================] - 0s 202us/sample - loss: 0.0310\n",
      "Epoch 2/10\n",
      "1870/1870 [==============================] - 0s 52us/sample - loss: 0.0211\n",
      "Epoch 3/10\n",
      "1870/1870 [==============================] - 0s 52us/sample - loss: 0.0194\n",
      "Epoch 4/10\n",
      "1870/1870 [==============================] - 0s 52us/sample - loss: 0.0193\n",
      "Epoch 5/10\n",
      "1870/1870 [==============================] - 0s 45us/sample - loss: 0.0196\n",
      "Epoch 6/10\n",
      "1870/1870 [==============================] - 0s 50us/sample - loss: 0.0178\n",
      "Epoch 7/10\n",
      "1870/1870 [==============================] - 0s 45us/sample - loss: 0.0162\n",
      "Epoch 8/10\n",
      "1870/1870 [==============================] - 0s 56us/sample - loss: 0.0242\n",
      "Epoch 9/10\n",
      "1870/1870 [==============================] - 0s 47us/sample - loss: 0.0217\n",
      "Epoch 10/10\n",
      "1870/1870 [==============================] - 0s 66us/sample - loss: 0.0135\n",
      "Train on 885 samples\n",
      "Epoch 1/10\n",
      "885/885 [==============================] - 0s 397us/sample - loss: 0.0218\n",
      "Epoch 2/10\n",
      "885/885 [==============================] - 0s 51us/sample - loss: 0.0179\n",
      "Epoch 3/10\n",
      "885/885 [==============================] - 0s 59us/sample - loss: 0.0258\n",
      "Epoch 4/10\n",
      "885/885 [==============================] - 0s 57us/sample - loss: 0.0213\n",
      "Epoch 5/10\n",
      "885/885 [==============================] - 0s 48us/sample - loss: 0.0178\n",
      "Epoch 6/10\n",
      "885/885 [==============================] - 0s 64us/sample - loss: 0.0152\n",
      "Epoch 7/10\n",
      "885/885 [==============================] - 0s 52us/sample - loss: 0.0173\n",
      "Epoch 8/10\n",
      "885/885 [==============================] - 0s 59us/sample - loss: 0.0122\n",
      "Epoch 9/10\n",
      "885/885 [==============================] - 0s 49us/sample - loss: 0.0085\n",
      "Epoch 10/10\n",
      "885/885 [==============================] - 0s 47us/sample - loss: 0.0111\n",
      "Train on 1530 samples\n",
      "Epoch 1/10\n",
      "1530/1530 [==============================] - 0s 267us/sample - loss: 0.0319\n",
      "Epoch 2/10\n",
      "1530/1530 [==============================] - 0s 59us/sample - loss: 0.0260\n",
      "Epoch 3/10\n",
      "1530/1530 [==============================] - 0s 59us/sample - loss: 0.0299\n",
      "Epoch 4/10\n",
      "1530/1530 [==============================] - 0s 51us/sample - loss: 0.0300\n",
      "Epoch 5/10\n",
      "1530/1530 [==============================] - 0s 60us/sample - loss: 0.0183\n",
      "Epoch 6/10\n",
      "1530/1530 [==============================] - 0s 55us/sample - loss: 0.0216\n",
      "Epoch 7/10\n",
      "1530/1530 [==============================] - 0s 57us/sample - loss: 0.0186\n",
      "Epoch 8/10\n",
      "1530/1530 [==============================] - 0s 54us/sample - loss: 0.0262\n",
      "Epoch 9/10\n",
      "1530/1530 [==============================] - 0s 56us/sample - loss: 0.0237\n",
      "Epoch 10/10\n",
      "1530/1530 [==============================] - 0s 88us/sample - loss: 0.0195\n",
      "Train on 336 samples\n",
      "Epoch 1/10\n",
      "336/336 [==============================] - 0s 1ms/sample - loss: 0.0106\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 0s 74us/sample - loss: 0.0113\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 0s 70us/sample - loss: 0.0050\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 0s 113us/sample - loss: 0.0050\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 0s 68us/sample - loss: 0.0022\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 0s 85us/sample - loss: 0.0065\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 0s 63us/sample - loss: 0.0046\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 0s 90us/sample - loss: 0.0023\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 0s 106us/sample - loss: 0.0025\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 0s 89us/sample - loss: 0.0035\n",
      "Train on 931 samples\n",
      "Epoch 1/10\n",
      "931/931 [==============================] - 0s 441us/sample - loss: 0.0330\n",
      "Epoch 2/10\n",
      "931/931 [==============================] - 0s 52us/sample - loss: 0.0128\n",
      "Epoch 3/10\n",
      "931/931 [==============================] - 0s 54us/sample - loss: 0.0083\n",
      "Epoch 4/10\n",
      "931/931 [==============================] - 0s 54us/sample - loss: 0.0070\n",
      "Epoch 5/10\n",
      "931/931 [==============================] - 0s 51us/sample - loss: 0.0069\n",
      "Epoch 6/10\n",
      "931/931 [==============================] - 0s 50us/sample - loss: 0.0057\n",
      "Epoch 7/10\n",
      "931/931 [==============================] - 0s 60us/sample - loss: 0.0110\n",
      "Epoch 8/10\n",
      "931/931 [==============================] - 0s 47us/sample - loss: 0.0077\n",
      "Epoch 9/10\n",
      "931/931 [==============================] - 0s 64us/sample - loss: 0.0071\n",
      "Epoch 10/10\n",
      "931/931 [==============================] - 0s 49us/sample - loss: 0.0103\n",
      "Train on 387 samples\n",
      "Epoch 1/10\n",
      "387/387 [==============================] - 0s 1ms/sample - loss: 0.0364\n",
      "Epoch 2/10\n",
      "387/387 [==============================] - 0s 90us/sample - loss: 0.0304\n",
      "Epoch 3/10\n",
      "387/387 [==============================] - 0s 84us/sample - loss: 0.0129\n",
      "Epoch 4/10\n",
      "387/387 [==============================] - 0s 62us/sample - loss: 0.0090\n",
      "Epoch 5/10\n",
      "387/387 [==============================] - 0s 108us/sample - loss: 0.0066\n",
      "Epoch 6/10\n",
      "387/387 [==============================] - 0s 56us/sample - loss: 0.0055\n",
      "Epoch 7/10\n",
      "387/387 [==============================] - 0s 80us/sample - loss: 0.0106\n",
      "Epoch 8/10\n",
      "387/387 [==============================] - 0s 103us/sample - loss: 0.0136\n",
      "Epoch 9/10\n",
      "387/387 [==============================] - 0s 64us/sample - loss: 0.0055\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 0s 51us/sample - loss: 0.0034\n",
      "Train on 1362 samples\n",
      "Epoch 1/10\n",
      "1362/1362 [==============================] - 0s 342us/sample - loss: 0.0309\n",
      "Epoch 2/10\n",
      "1362/1362 [==============================] - 0s 55us/sample - loss: 0.0088\n",
      "Epoch 3/10\n",
      "1362/1362 [==============================] - 0s 55us/sample - loss: 0.0072\n",
      "Epoch 4/10\n",
      "1362/1362 [==============================] - 0s 51us/sample - loss: 0.0150\n",
      "Epoch 5/10\n",
      "1362/1362 [==============================] - 0s 51us/sample - loss: 0.0184\n",
      "Epoch 6/10\n",
      "1362/1362 [==============================] - 0s 58us/sample - loss: 0.0078\n",
      "Epoch 7/10\n",
      "1362/1362 [==============================] - 0s 53us/sample - loss: 0.0124\n",
      "Epoch 8/10\n",
      "1362/1362 [==============================] - 0s 51us/sample - loss: 0.0079\n",
      "Epoch 9/10\n",
      "1362/1362 [==============================] - 0s 84us/sample - loss: 0.0087\n",
      "Epoch 10/10\n",
      "1362/1362 [==============================] - 0s 84us/sample - loss: 0.0115\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([tf.keras.layers.Dense(64,input_shape=(4,),activation='relu'),\n",
    "                             tf.keras.layers.Dense(128,activation='relu'),\n",
    "                             tf.keras.layers.Dense(2, activation='softmax'),\n",
    "\n",
    "])\n",
    "final_score_list = params['score']\n",
    "for trial in range(TRIALS):    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "    if params['X'].size > 0:\n",
    "        model.fit(params['X'],params['Y'], epochs=10)\n",
    "    else:\n",
    "        break\n",
    "    params = dict(\n",
    "        goal_steps = 500,\n",
    "        score_requirement = 50 + trial * 20,\n",
    "        initial_games = 100,\n",
    "        X = np.array([[]]),\n",
    "        Y = np.array([]),\n",
    "        action = 2,\n",
    "        score = np.array([0]),\n",
    "        plot = False,\n",
    "        model = model\n",
    "    )\n",
    "    data(params)\n",
    "    final_score_list = np.concatenate((final_score_list, params['score']), axis=None)\n",
    "plot(final_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
